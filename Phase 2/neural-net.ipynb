{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages used to handle files\n",
    "import sys\n",
    "import os \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import tensorflow\n",
    "# commonly used library for data manipilation\n",
    "\n",
    "import pandas\n",
    "# numerical\n",
    "import numpy as np\n",
    "\n",
    "# handle images - opencv\n",
    "import cv2\n",
    "\n",
    "# machine learning library\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "#used to serialize python objects to disk and load them back to memory\n",
    "import pickle\n",
    "\n",
    "# helper functions kindly provided for you by Matthias \n",
    "import helpers\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "# tell matplotlib that we plot in a notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "# filepath constants\n",
    "DATA_BASE_PATH = '../individual/data/data/'\n",
    "OUTPUT_PATH='./'\n",
    "PREDICTION_PATH = os.path.join(OUTPUT_PATH,'predictions')\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_BASE_PATH,'train')\n",
    "DATA_TEST_PATH = os.path.join(DATA_BASE_PATH,'test')\n",
    "DATA_TEST_GROUPED_PATH = os.path.join(DATA_BASE_PATH,'test_grouped')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths=glob.glob(os.path.join(DATA_TRAIN_PATH,'*'))\n",
    "label_strings = np.sort(np.array([os.path.basename(path) for path in folder_paths]))\n",
    "num_classes = label_strings.shape[0]\n",
    "print(label_strings)\n",
    "print(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = dict((label_string, helpers.getImgPaths(os.path.join(DATA_TRAIN_PATH,label_string))) for label_string in label_strings)\n",
    "test_grouped_paths = dict((label_string, helpers.getImgPaths(os.path.join(DATA_TEST_GROUPED_PATH,label_string))) for label_string in label_strings)\n",
    "\n",
    "test_paths = helpers.getImgPaths(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "\n",
    "def maskForeground(image_array):\n",
    "    image_array_downsample = np.zeros((int(image_array.shape[1]/2), int(image_array.shape[0]/2)))\n",
    "    image_array_downsample = cv2.resize(image_array, image_array_downsample.shape, interpolation=cv2.INTER_LINEAR) \n",
    "\n",
    "    mask_downsample = np.zeros(image_array_downsample.shape[:2],np.uint8)\n",
    "\n",
    "    bgdModel = np.zeros((1,65),np.float64)\n",
    "    fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "    rect = (25, 25, image_array.shape[1] - 50, image_array.shape[0] - 50)\n",
    "\n",
    "    cv2.grabCut(image_array_downsample,mask_downsample,rect,bgdModel,fgdModel,5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "    mask_downsample = np.where((mask_downsample==2)|(mask_downsample==0),0,1).astype('uint8')\n",
    "    mask = np.zeros_like(image_array.shape[:2])\n",
    "    mask = cv2.resize(mask_downsample, image_array.shape[:2][::-1], interpolation=cv2.INTER_LINEAR) \n",
    "\n",
    "    thresh = np.sum(mask)/(mask.shape[0] * mask.shape[1])\n",
    "\n",
    "    if thresh < 0.05 or thresh > 0.55:\n",
    "        mask = None\n",
    "    else:\n",
    "        mask = mask[:,:,np.newaxis]\n",
    "        mask = mask.reshape(mask.shape[0], mask.shape[1])\n",
    "        \n",
    "    return image_array, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def process_image_with_background(image_path):\n",
    "        \n",
    "    im = Image.open(image_path)\n",
    "    im = np.array(im.resize((256,256), Image.ANTIALIAS))\n",
    "    return np.array(im/255,dtype='f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def process_image(image_path,remove_background=True):\n",
    "        \n",
    "    im = Image.open(image_path)\n",
    "    im = np.array(im.resize((256,256), Image.ANTIALIAS))\n",
    "    \n",
    "    if not remove_background:\n",
    "        return np.array(im/255,dtype='f')\n",
    "        \n",
    "    img, mask = maskForeground(im)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    if mask is None:\n",
    "        masked_img = img\n",
    "    else:\n",
    "        masked_img = img * mask[:,:,np.newaxis]\n",
    "\n",
    "    X = np.array(masked_img/255,dtype='f')\n",
    "    with_background = np.array(im/255,dtype='f')\n",
    "    return (X,with_background)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for animal in label_strings:\n",
    "    paths = train_paths[animal]\n",
    "    \n",
    "    pool = ThreadPool(8) \n",
    "    results = pool.map(process_image, paths)\n",
    "    \n",
    "    for result in results:\n",
    "        if result is not None: \n",
    "            X.append(result[1])\n",
    "            y.append(animal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for animal in label_strings:\n",
    "    paths = test_grouped_paths[animal]\n",
    "    \n",
    "    pool = ThreadPool(8) \n",
    "    results = pool.map(process_image_with_background, paths)\n",
    "    \n",
    "    for result in results:\n",
    "        if result is not None: \n",
    "            X_test.append(result)\n",
    "            y_test.append(animal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a labelencoder to obtain numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_labels = y\n",
    "\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 123456,\n",
    "                                                    test_size = 0.3\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to categorial arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(y)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "datagen=ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=True, rotation_range=0.10,  width_shift_range=0.2, height_shift_range=0.2, horizontal_flip = True, vertical_flip = False,fill_mode='nearest',shear_range=0.2)\n",
    "train_generator=datagen.flow(X_train,y_train,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "test_datagen=ImageDataGenerator()\n",
    "test_datagen.fit(X_test)\n",
    "validation_generator=test_datagen.flow(X_test,y_test,batch_size=batch_size)\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(256,256,3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y),\n",
    "                                                 y) \n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_steps=len(X_test)//batch_size,\n",
    "        steps_per_epoch=len(X_train)//batch_size*3,\n",
    "        validation_data=validation_generator,\n",
    "        workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_final.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_final.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "import keras\n",
    "json_file = open('model_final.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_final.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "model = loaded_model\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.0008)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "X_test = []\n",
    "for image in test_paths:\n",
    "    im = Image.open(image)\n",
    "    im = im.resize((256,256), Image.ANTIALIAS)\n",
    "    X_test.append(np.array(im,dtype='f')/255)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred = model.predict_proba(np.array(X_test))\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1),np.argmax(pred, axis=1) )\n",
    "\n",
    "cm_nor = np.matrix((cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]).round(decimals=2))\n",
    "df_cm = pd.DataFrame(cm_nor, label_strings,\n",
    "                  label_strings)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, cmap=\"Blues\")# font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a submission\n",
    "pred_file_path = os.path.join(PREDICTION_PATH, helpers.generateUniqueFilename('prediction','csv'))\n",
    "helpers.writePredictionsToCsv(predictions,pred_file_path,label_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
